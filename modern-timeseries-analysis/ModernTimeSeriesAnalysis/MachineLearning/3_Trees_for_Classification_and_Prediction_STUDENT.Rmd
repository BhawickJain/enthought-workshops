---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.11.1
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [10, 10]
```

```{python}
import cesium
import xgboost as xgb
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import time

from cesium import datasets
from cesium import featurize as ft

import scipy
from scipy.stats import pearsonr, spearmanr
from scipy.stats import skew

import sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
```

```{python}
print(cesium.__version__)
print(xgb.__version__)
print(scipy.__version__)
print(sklearn.__version__)
```

## Load data and generate some features of interest

```{python}
eeg = datasets.fetch_andrzejak()
```

```{python}
type(eeg)
```

```{python}
eeg.keys()
```

### Visually inspect

```{python}
plt.subplot(3, 1, 1)
plt.plot(eeg["measurements"][0])
plt.legend(eeg['classes'][0])
plt.subplot(3, 1, 2)
plt.plot(eeg["measurements"][300])
plt.legend(eeg['classes'][300])
plt.subplot(3, 1, 3)
plt.plot(eeg["measurements"][450])
plt.legend(eeg['classes'][450])
```

```{python}
type(eeg["measurements"][0])
```

```{python}
type(eeg)
```

```{python}
eeg.keys()
```

```{python}
type(eeg['measurements'])
```

```{python}
len(eeg['measurements'])
```

```{python}
eeg['measurements'][0].shape
```

## Generate the features

```{python}
# from cesium import featurize as ft
# features_to_use = ["amplitude",
#                    "percent_beyond_1_std",
#                    "percent_close_to_median",
#                   "skew",
#                   "max_slope"]
# fset_cesium = ft.featurize_time_series(times=eeg["times"],
#                                               values=eeg["measurements"],
#                                               errors=None,
#                                               features_to_use=features_to_use,
#                                              scheduler = None)
```

```{python}
fset_cesium = pd.read_csv("data/full_eeg_data_features.csv", header = [0, 1])
```

```{python}
fset_cesium.head()
```

```{python}
# fset_cesium.to_csv("full_eeg_data_features.csv")
```

```{python}
fset_cesium.shape
```

## Exercise: validate/calculate these features by hand
#### look up feature definitions here: http://cesium-ml.org/docs/feature_table.html
confirm the values by hand coding these features for the first EEG measurement
(that is eeg["measurements"][0])

```{python}
ex = eeg["measurements"][0]
```

```{python}
ex_mean = np.mean(ex)
ex_std  = np.std(ex)
```

```{python}
# amplitude
(np.max(ex) - np.min(ex)) / 2
```

```{python}
siz = len(ex)
ll = ex_mean - ex_std
ul = ex_mean + ex_std

quals = [i for i in range(siz) if ex[i] < ll or ex[i] > ul]
len(quals)/len(ex)
```

```{python}
# percent_close to median
# Percentage of values within wndow_frac*(max(x)-min(x)) of median.
# window frac = 0.1
window = 0.1 * (np.max(ex) -  np.min(ex))
np.where(np.abs(ex_mean - ex) < window)[0].shape[0] / ex.shape[0]
```

```{python}
# skew
print(skew(ex))
plt.hist(ex)
```

```{python}
## max slope
times = eeg["times"][0]
np.max(np.abs(np.diff(ex)/np.diff(times)))
```

```{python}
plt.hist(fset_cesium.iloc[:, 1])
```

```{python}
fset_cesium['classes'] = eeg['classes']
```

```{python}
fset_cesium.columns = fset_cesium.columns.droplevel(-1)
```

```{python}

```

```{python}
fset_cesium.groupby('classes')['amplitude'].hist()
```

```{python}
fset_cesium['amplitude'].hist(by=fset_cesium['classes'])
```

```{python}
fset_cesium['max_slope'].hist(by=fset_cesium['classes'])
```

## Prepare data for training

```{python}
X_train, X_test, y_train, y_test = train_test_split(
     fset_cesium.iloc[:, 1:6].values, eeg["classes"], random_state=21)
```

## Try a random forest with these features

```{python}
clf = RandomForestClassifier(n_estimators=10, max_depth=3,
                              random_state=21)
```

```{python}
clf.fit(X_train, y_train)
```

```{python}
clf.score(X_train, y_train)
```

```{python}
clf.score(X_test, y_test)
```

```{python}
np.unique(y_test, return_counts=True)
```

```{python}
y_test
```

```{python}
y_test.shape
```

```{python}
y_train.shape
```

## Try XGBoost with these features

```{python}
model = xgb.XGBClassifier(n_estimators=10, max_depth=3,
                              random_state=21)
model.fit(X_train, y_train)
```

```{python}
model.score(X_test, y_test)
```

```{python}
model.score(X_train, y_train)
```

```{python}
xgb.plot_importance(model)
```

## Time Series Forecasting with Decision Trees


### Explore the data

```{python}
ap = pd.read_csv("data/AirPassengers.csv", parse_dates=[0])
```

```{python}
ap.head()
```

```{python}
ap.set_index('Month', inplace=True)
```

```{python}
ap.head()
```

```{python}
plt.plot(ap)
```

`[?]` not sure what is going on. The transformation below enables the machine learning model to better compare the values across the time series. You can see the time dependent dynamics of increasing passenger numbers has been removed, enabling the focus on predicting the more complicated pattern.

```{python}
plt.plot(np.diff(np.log(ap.values[:, 0])))
```

```{python}
ts = np.diff(np.log(ap.values[:, 0]))
```

## Exercise: now that we have 1 time series, how can we convert it to many samples?

```{python}
NSTEPS = 12
```

```{python}
ts.shape
```

```{python}
vals = np.hstack([np.expand_dims(np.array(ts, dtype=np.float32), axis=1) for _ in range(NSTEPS)])
```

```{python}
vals.shape
```

```{python}
ts[0:NSTEPS]
```

```{python}
vals.shape
```

```{python}
nrows = vals.shape[0]
for lag in range(1, vals.shape[1]):
    vals[:(nrows-lag),lag] = vals[lag:, lag]
    vals[(nrows-lag):, lag] = np.nan
```

```{python}
vals
```

```{python}
vals = vals[:(vals.shape[0] - NSTEPS +1), :]
```

```{python}
vals.shape
```

```{python}
vals[-1]
```

```{python}
vals[-2]
```

the values within the `vals[-1]` and `vals[-2]` should overlap, in my computation they do not which could investigate.

```{python}
ts[-NSTEPS:]
```

```{python}
vals.shape
```

## Exercise: now that we have the time series broken down into a set of samples, how to featurize?

```{python}
measures = [vals[i][0:(NSTEPS - 1)] for i in range(vals.shape[0])]
```

```{python}
times = [[j for j in range(NSTEPS - 1)] for i in range(vals.shape[0])]
```

```{python}
measures[0]
```

```{python}
len(measures[0])
```

```{python}
features_to_use = [
                    "amplitude",
                    "percent_beyond_1_std",
                    "skew",
                    "max_slope",
                    "percent_amplitude"]

fset_ap = ft.featurize_time_series(times=times,
                                  values=measures,
                                  errors=None,
                                  features_to_use=features_to_use,
                                  scheduler=None)
```

```{python}
fset_ap.columns = fset_ap.columns.droplevel(-1)
```

```{python}
fset_ap.head()
```

```{python}
plt.hist(fset_ap.amplitude)
```

```{python}
plt.hist(fset_ap.percent_amplitude)
```

```{python}
plt.hist(fset_ap['skew'])
```

## Exercise: can you fit an XGBRegressor to this problem? Let's use the first 100 'time series' as the training data

```{python}
outcomes = vals[:, -1]
```

```{python}
X_train, y_train = fset_ap.iloc[:100, :], outcomes[:100]
X_test, y_test = fset_ap.iloc[100:, :], outcomes[100:]
```

```{python}
X_train.shape
```

```{python}
model = xgb.XGBRegressor(n_estimates=20, max_depth=2, random_state=21)
```

```{python}
eval_set = [(X_test, y_test)]
model.fit(X_train, y_train, eval_metric='rmse', eval_set=eval_set, verbose=True)
```

### RMSE can be hard to digest .... Use other assessments to determine how well the model performs

```{python}
plt.scatter(model.predict(X_test), y_test)
```

```{python}
plt.scatter(model.predict(X_train), y_train)
```

```{python}
pearsonr(model.predict(X_train), y_train)
```

```{python}
pearsonr(model.predict(X_test), y_test)
```

```{python}
xgb.plot_importance(model)
```

### What went wrong? Let's revisit the feature set

```{python}
fset_ap.head()
```

```{python}
plt.plot(vals[0])
plt.plot(vals[1])
plt.plot(vals[2])
```

## We need to find a way to generate features that encode positional information


### now we will generate our own features

```{python}
vals.shape
```

```{python}
feats = np.zeros( (vals.shape[0], 6), dtype = np.float32)
for i in range(vals.shape[0]):
    feats[i, 0] = np.where(vals[i] == np.max(vals[i]))[0][0]
    feats[i, 1] = np.where(vals[i] == np.min(vals[i]))[0][0]
    feats[i, 2] = feats[i, 0] - feats[i, 1]
    feats[i, 3] = np.max(vals[i][-3:])
    feats[i, 4] = vals[i][-1] - vals[i][-2]
    feats[i, 5] = vals[i][-1] - vals[i][-3]
```

```{python}
feats[0:3]
```

### How do these look compared to the first set of features?

```{python}
pd.DataFrame(feats[0:3])
```

```{python}
X_train, y_train = feats[:100, :], outcomes[:100]
X_test, y_test   = feats[100:, :], outcomes[100:]
```

```{python}
model = xgb.XGBRegressor(n_estimators=20, max_depth=2,
                              random_state=21)
eval_set = [(X_test, y_test)]
model.fit(X_train, y_train, eval_metric="rmse", eval_set=eval_set, verbose=True)
```

```{python}
plt.scatter(model.predict(X_test), y_test)
```

```{python}
print(pearsonr(model.predict(X_test), y_test))
print(spearmanr(model.predict(X_test), y_test))
```

```{python}
plt.scatter(model.predict(X_train), y_train)
```

```{python}
print(pearsonr(model.predict(X_train), y_train))
print(spearmanr(model.predict(X_train), y_train))
```
