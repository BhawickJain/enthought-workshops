---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.11.1
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
# %matplotlib inline
import matplotlib
matplotlib.rcParams['figure.figsize'] = [8, 3]
import matplotlib.pyplot as plt

import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels

import scipy
from scipy.stats import pearsonr

from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()
```

```{python}
print(matplotlib.__version__)
print(pd.__version__)
print(np.__version__)
print(statsmodels.__version__)
print(scipy.__version__)

```

## Obtain and visualize data

```{python}
## data obtained from https://datahub.io/core/global-temp#data
df = pd.read_csv("global_temps.csv")
df.head()
```

```{python}
df.Mean[:100].plot()
```

```{python}
df_gcag = df[df['Source']=='GCAG']
df_gcag
```

```{python}
df_gistemp = df[df['Source']=='GISTEMP']
df_gistemp
```

```{python}
df_gcag.Mean[:100].plot()
```

## Exercise: what is wrong with the data and plot above? How can we fix this?


1. two different sources are being charted on the same graph with no legends.
2. missing units
3. x axis is appears to be the day number
4. x axis time series move forwards from right to left

```{python}
df = df.pivot(index='Date', columns='Source', values='Mean')
```

```{python}
df.head()
```

```{python}
df.GCAG.plot()
```

```{python}
type(df.index)
```

## Exercise: how can we make the index more time aware?

```{python}
df.index = pd.to_datetime(df.index)
```

```{python}
type(df.index)
```

```{python}
df.GCAG.plot()
```

```{python}
df['1880']
```

```{python}
plt.plot(df['1880':'1950'][['GCAG', 'GISTEMP']])
```

```{python}
plt.plot(df['1950':][['GISTEMP']])
```

## Exercise: How strongly do these measurements correlate contemporaneously? What about with a time lag?


You can use a scatter plot to snippets of time for each source

```{python}
plt.scatter(df['1880':'1900'][['GCAG']], df['1880':'1900'][['GISTEMP']])
```

```{python}
plt.scatter(df['1880':'1899'][['GCAG']], df['1881':'1900'][['GISTEMP']])
```

```{python}
pearsonr(df['1880':'1899'].GCAG, df['1881':'1900'].GISTEMP)
```

```{python}
df['1880':'1899'][['GCAG']].head()
```

```{python}
df['1881':'1990'][['GISTEMP']].head()
```

```{python}
min(df.index)
```

```{python}
max(df.index)
```

## Unobserved component model

```{python}
train = df['1960':]
```

### model parameters

```{python}
# smooth trend model without seasonal or cyclical components
model = {
    'level': 'smooth trend', 'cycle': False, 'seasonal': None, 
}

```

### fitting a model

```{python}
# https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.structural.UnobservedComponents.html
gcag_mod = sm.tsa.UnobservedComponents(train['GCAG'], **model)
gcag_res = gcag_mod.fit()
```

```{python}
fig = gcag_res.plot_components(legend_loc='lower right', figsize=(15, 9));
```

## Plotting predictions

```{python}
# Perform rolling prediction and multistep forecast
num_steps = 20
predict_res = gcag_res.get_prediction(dynamic=train['GCAG'].shape[0] - num_steps)

predict = predict_res.predicted_mean
ci = predict_res.conf_int()
```

```{python}
plt.plot(predict)
```

```{python}
plt.scatter(train['GCAG'], predict)
```

```{python}
fig, ax = plt.subplots()
# Plot the results
ax.plot(train['GCAG'], 'k.', label='Observations');
ax.plot(train.index[:-num_steps], predict[:-num_steps], label='One-step-ahead Prediction');

ax.plot(train.index[-num_steps:], predict[-num_steps:], 'r', label='Multistep Prediction');
ax.plot(train.index[-num_steps:], ci.iloc[-num_steps:], 'k--');

# Cleanup the image
legend = ax.legend(loc='upper left');
```

```{python}
fig, ax = plt.subplots()
# Plot the results
ax.plot(train.index[-40:], train['GCAG'][-40:], 'k.', label='Observations');
ax.plot(train.index[-40:-num_steps], predict[-40:-num_steps], label='One-step-ahead Prediction');

ax.plot(train.index[-num_steps:], predict[-num_steps:], 'r', label='Multistep Prediction');
ax.plot(train.index[-num_steps:], ci.iloc[-num_steps:], 'k--');

# Cleanup the image
legend = ax.legend(loc='upper left');
```

## Exercise: consider adding a seasonal term for 12 periods for the model fit above. Does this improve the fit of the model?

```{python}
seasonal_model = {
    'level': 'local linear trend', 'seasonal': 12,
}

mod = sm.tsa.UnobservedComponents(train['GCAG'], **seasonal_model)
res = mod.fit(method='powell', disp=False)
```

```{python}
fig = res.plot_components(legend_loc='lower right', figsize=(15, 9));
```

`[?]` What is the difference between a Level component and the Trent Component?


## How does this compare to the original model?

```{python}
pearsonr(gcag_res.predict(), train['GCAG'])
```

```{python}
pearsonr(res.predict(), train['GCAG'])
```

You can see that inspite of the above STS model doing a much better job (at least visually) fitting to the data, the pearsons rank correlation are not so different. This is an example of how numeric comparison can fall short, and models with graphs can be helpful.

```{python}
np.mean(np.abs(gcag_res.predict() - train['GCAG']))
```

```{python}
np.mean(np.abs(res.predict() - train['GCAG']))
```

## Let's explore the seasonality more

```{python}
seasonal_model = {
    'level': 'local level',
    'seasonal': 12
}
llmod = sm.tsa.UnobservedComponents(train['GCAG'], **seasonal_model)
ll_level_res = llmod.fit(method='powell', disp=False)
```

```{python}
fig = ll_level_res.plot_components(legend_loc='lower right', figsize=(15, 9));
```

The Structural Time Series model is a great for building intuition and getting a first order understanding of the data compared to something like an ARIMA model. This is because STS can breakdown the data into 'components' which we can adjust using the domain knowledge. In the above exercise we have used a seasonal component which has 12 parts, each representing months. 


STS has managed to nicely take out the seasons and also show the increasing level of global temperature data which can inform out intuition and the arguments we can potentially make. ARIMA on the other hand would only provide cofficients which are difficult to hold any intuition about.


The instructor generally does not use STS in making real decisions but instead uses it as a tool to understand the data and capture intuition which other more sophisticated models may not afford. STS is very easy to overfit.


`[?]` What is the difference between a cycle and a season?  
`[>]` Seasons are more regular than a cycle. The Seasonal component has $n-1$ coefficients, $n$ is the number of seasons declared. These are adjusted to fit the trend. In a cyclical model, a sinosoidal wave is used instead, the frequency is determined to fit the data. It can also hold a damping parameter and the frequency itself can be time dependent.

```{python}
np.mean(np.abs(ll_level_res.predict() - train['GCAG']))
```

```{python}
train[:48].GCAG.plot()
```

## Exercise: a common null model for time series is to predict the value at time t-1 for the value at time t. How does such a model compare to the models we fit here?


### Consider correlation

```{python}
pearsonr(ll_level_res.predict(), train['GCAG'])
```

```{python}
pearsonr(train['GCAG'].iloc[:-1, ], train['GCAG'].iloc[1:, ])
```

### What about mean absolute error?

```{python}
np.mean(np.abs(ll_level_res.predict() - train['GCAG']))
```

```{python}
np.mean(np.abs(train['GCAG'].iloc[:-1, ].values, train['GCAG'].iloc[1:, ].values))
```

```{python}

```
